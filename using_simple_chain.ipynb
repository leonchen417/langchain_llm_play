{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851f9ec4-2caa-4484-9d18-ca00d0cf6ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/3nrys0b97rg31_kvjlgmc7w40000gn/T/ipykernel_966/2884609304.py:9: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llama_3 = HuggingFaceHub(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Initialize LLaMA models from HuggingFaceHub (replace with actual repo ID and API key)\n",
    "llama_3 = HuggingFaceHub(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "#llama_2 = HuggingFaceHub(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0e890b-f76f-4dec-9a50-a626a90d2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "def prompt_builder(model_type=\"llama-3\", system_prompt=\"\", context=\"\", prompt_data=\"\"):\n",
    "\n",
    "    if \"llama-2\" in model_type:\n",
    "        template = \"\"\"<s>[INST] <<SYS>> {system_prompt} <</SYS>> {context} {prompt_data}  [/INST] \"\"\"\n",
    "    elif \"llama-3\" in model_type:\n",
    "        template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|> {context} {prompt_data} <|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|> \"\"\"\n",
    "    elif \"mixtral\" in model_type:\n",
    "        template = \"\"\"<s> [INST] {system_prompt} {context} {prompt_data} [/INST] \"\"\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Can not work for {model_type=}\")\n",
    "\n",
    "    full_template = template.format(system_prompt=system_prompt, context=context, prompt_data=prompt_data)\n",
    "    prompt = PromptTemplate(\n",
    "        template=full_template,)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d048ba5e-ab58-4d2d-be62-60b5ac6f7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a5eb5b-0ad9-4733-9ad4-0df11b90743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_percentage = df.discount_percentage\n",
    "rating = df.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d147a4e6-96b7-4c35-a930-bdc0a8b65a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.2',\n",
       " '4.0',\n",
       " '3.9',\n",
       " '4.2',\n",
       " '4.2',\n",
       " '3.9',\n",
       " '4.1',\n",
       " '4.3',\n",
       " '4.2',\n",
       " '4.0',\n",
       " '4.3',\n",
       " '4.2',\n",
       " '4.4',\n",
       " '4.2',\n",
       " '4.1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rating[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10da353d-23e9-4caf-bbbe-79ae8ad12d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/3nrys0b97rg31_kvjlgmc7w40000gn/T/ipykernel_966/1516905712.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain_1 = LLMChain(llm=llama_3, prompt=prompt_1, output_key=\"Review_status\")\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates for both models\n",
    "system_prompt_template_1 = \"You are a online store assistant. Classifying if an item is [Good, Fair, Bad, Very Bad] in <classify></classify> tag.\"\n",
    "template_1 = \"\"\"\n",
    "If an item get bad review on a discount, it's a bad item.\n",
    "Here are some examples:\n",
    "1. discount percentage: 62% and rating: 2.2 is \"Very Bad\".\n",
    "2. discount percentage: 30% and rating: 2.2 is \"Bad\".\n",
    "3. discount percentage: 30% and rating 4 is \"Fair\".\n",
    "4. discount percentage: 10% and rating: 4.5 is \"Good\".\n",
    "Now given discount percentage: {discount_percentage} and rating: {rating}, give only a 1 word review in [Good, Fair, Bad, Very Bad].\n",
    "\"\"\"\n",
    "# prompt_1 = ChatPromptTemplate.from_template(template_1)\n",
    "prompt_1 = prompt_builder(system_prompt=system_prompt_template_1, prompt_data=template_1)\n",
    "print(type(prompt_1))\n",
    "chain_1 = LLMChain(llm=llama_3, prompt=prompt_1, output_key=\"Review_status\")\n",
    "\n",
    "\n",
    "system_prompt_template_2 = \"You are a helpful text assistant. Get the value within the <classify></classify> tags and output\"\n",
    "template_2= \"\"\"\n",
    "Here are some example:\n",
    "1. Random descrption that is not related to the task <classify>\"Very Bad\"</classify> and you just get the value in the tags:\"Very Bad\".\n",
    "2. <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a online store sales assistant. Given the item is claissified as Good or Bad, decide if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|> <classify>\"Bad\"</classify>:\"Bad\"\n",
    "3. if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nIf an item is Bad or Very Bad, we don\\'t want to sell it anymore.\\nHere <classify>\"Good\"</classify>:\"Good\"\n",
    "\"\"\"\n",
    "prompt_2 = prompt_builder(system_prompt=system_prompt_template_2, prompt_data=template_2)\n",
    "chain_2 = LLMChain(llm=llama_3, prompt=prompt_2, output_key=\"Review_status_short\")\n",
    "\n",
    "system_prompt_template_3 = \"You are a online store sales assistant. Given the item is claissified as Good or Bad, decide if we will keep selling them as [Yes, Neutral, No].\"\n",
    "\n",
    "template_3 = \"\"\"\n",
    "If an item is Bad or Very Bad, we don't want to sell it anymore.\n",
    "Here are some examples:\n",
    "1. Given the item is claissified as Very Bad, output: \"No\".\n",
    "2. Given the item is claissified as Bad, output: \"No\".\n",
    "3. Given the item is claissified as Fair, output: \"Neutral\".\n",
    "4. Given the item is claissified as Good, output: \"Yes\".\n",
    "Now given the item is claissified as {Review_status_short}, output: \n",
    "\"\"\"\n",
    "\n",
    "prompt_3 = prompt_builder(system_prompt=system_prompt_template_3, prompt_data=template_3)\n",
    "chain_3 = LLMChain(llm=llama_3, prompt=prompt_3, output_key=\"is_keeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67999cc5-878b-41b4-b6ae-d47528e234ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/3nrys0b97rg31_kvjlgmc7w40000gn/T/ipykernel_966/2078259296.py:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res2 = overall_sequential_chain({\"discount_percentage\":my_discount[4], \"rating\":my_rating[4]})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Input text to be passed to the first chain\\nfor each_discount_percentage, each_rating in zip(my_discount, my_rating):\\n    # Run the sequential chain\\n    final_output = sequential_chain.run({\"discount_percentage\": each_discount_percentage, \"rating\":each_rating})\\n    print(\"Final output from the sequential chain:\", final_output)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use SequentialChain to chain the two LLM chains\n",
    "overall_sequential_chain = SequentialChain(chains=[chain_1, chain_2, chain_3],\n",
    "                                           input_variables=[\"discount_percentage\", \"rating\"],\n",
    "                                           output_variables=[\"Review_status\",\"Review_status_short\", \"is_keeping\"],\n",
    "                                           verbose=True)\n",
    "# sequential_chain = SequentialChain(chains=[chain_1],input_variables=[\"discount_percentage\", \"rating\"], verbose=True)\n",
    "\n",
    "my_discount = list(discount_percentage[:15])\n",
    "my_rating = list(rating[:15]) \n",
    "res2 = overall_sequential_chain({\"discount_percentage\":my_discount[4], \"rating\":my_rating[4]})\n",
    "\"\"\"\n",
    "# Input text to be passed to the first chain\n",
    "for each_discount_percentage, each_rating in zip(my_discount, my_rating):\n",
    "    # Run the sequential chain\n",
    "    final_output = sequential_chain.run({\"discount_percentage\": each_discount_percentage, \"rating\":each_rating})\n",
    "    print(\"Final output from the sequential chain:\", final_output)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9555acfa-e98d-442a-b4d7-a919a0c89e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discount_percentage': '61%',\n",
       " 'rating': '4.2',\n",
       " 'Review_status': '<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a online store assistant. Classifying if an item is [Good, Fair, Bad, Very Bad] in <classify></classify> tag. <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nIf an item get bad review on a discount, it\\'s a bad item.\\nHere are some examples:\\n1. discount percentage: 62% and rating: 2.2 is \"Very Bad\".\\n2. discount percentage: 30% and rating: 2.2 is \"Bad\".\\n3. discount percentage: 30% and rating 4 is \"Fair\".\\n4. discount percentage: 10% and rating: 4.5 is \"Good\".\\nNow given discount percentage: 61% and rating: 4.2, give only a 1 word review in [Good, Fair, Bad, Very Bad].\\n <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>  <classify>Bad</classify>',\n",
       " 'Review_status_short': '<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful text assistant. Get the value within the <classify></classify> tags and output <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nHere are some example:\\n1. Random descrption that is not related to the task <classify>\"Very Bad\"</classify> and you just get the value in the tags:\"Very Bad\".\\n2. <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a online store sales assistant. Given the item is claissified as Good or Bad, decide if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|> <classify>\"Bad\"</classify>:\"Bad\"\\n3. if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nIf an item is Bad or Very Bad, we don\\'t want to sell it anymore.\\nHere <classify>\"Good\"</classify>:\"Good\"\\n <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|> ',\n",
       " 'is_keeping': '<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a online store sales assistant. Given the item is claissified as Good or Bad, decide if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nIf an item is Bad or Very Bad, we don\\'t want to sell it anymore.\\nHere are some examples:\\n1. Given the item is claissified as Very Bad, output: \"No\".\\n2. Given the item is claissified as Bad, output: \"No\".\\n3. Given the item is claissified as Fair, output: \"Neutral\".\\n4. Given the item is claissified as Good, output: \"Yes\".\\nNow given the item is claissified as <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful text assistant. Get the value within the <classify></classify> tags and output <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nHere are some example:\\n1. Random descrption that is not related to the task <classify>\"Very Bad\"</classify> and you just get the value in the tags:\"Very Bad\".\\n2. <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a online store sales assistant. Given the item is claissified as Good or Bad, decide if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|> <classify>\"Bad\"</classify>:\"Bad\"\\n3. if we will keep selling them as [Yes, Neutral, No]. <|eot_id|>\\n<|start_header_id|>user<|end_header_id|>  \\nIf an item is Bad or Very Bad, we don\\'t want to sell it anymore.\\nHere <classify>\"Good\"</classify>:\"Good\"\\n <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|> , output: \\n <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>  \"Yes\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae946b-b0ad-4d0e-bbf1-8c46415c9ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
